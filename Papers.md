Paper | Links | 
------------  |------
Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains |[Paper](https://arxiv.org/abs/2006.10739) & [Blog](https://bmild.github.io/fourfeat/index.html)
Perceiver: General Perception with Iterative Attention | [Paper](https://arxiv.org/abs/2103.03206)
Language Models are Few-Shot Learners | [Paper](https://arxiv.org/pdf/2005.14165.pdf) & [Blog](https://docs.cohere.ai/)
On the Spectral Bias of Neural Networks | [Paper](https://arxiv.org/pdf/1806.08734.pdf)
Shortcut Learning in Deep Neural Networks | [Paper](https://arxiv.org/pdf/2004.07780.pdf)
Gradient Starvation: A Learning Proclivity in Neural Networks | [Paper](https://arxiv.org/abs/2011.09468)
REALM: Retrieval-Augmented Language Model Pre-Training | [Paper](https://arxiv.org/abs/2002.08909)
Improving language models by retrieving from trillions of tokens | [Paper](https://arxiv.org/abs/2112.04426)
Extracting Training Data from Large Language Models | [Paper](https://arxiv.org/abs/2012.07805)
Masked Autoencoders Are Scalable Vision Learners |[Paper](https://arxiv.org/abs/2111.06377)
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [Paper](https://arxiv.org/abs/2010.11929)
Scaling Language Models: Methods, Analysis & Insights from Training Gopher | [Paper](https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf)
Inductive Biases for Deep Learning of Higher-Level Cognition | [Paper](https://arxiv.org/pdf/2011.15091.pdf)
Distilling the knowledge in a neural network | [Paper](https://arxiv.org/abs/1503.02531) & [Blog](https://wandb.ai/authors/knowledge-distillation/reports/Distilling-Knowledge-in-Neural-Networks--VmlldzoyMjkxODk) & [Video](https://www.youtube.com/watch?v=k63qGsH1jLo&t=986s) & [Another Blog](https://blog.floydhub.com/knowledge-distillation/)
Distilling Task-Specific Knowledge from BERT into Simple Neural Networks | [Paper](https://arxiv.org/pdf/1903.12136.pdf) & [Video](https://www.youtube.com/watch?v=Xji8NmL3FvQ&t=636s) & [Video](https://www.youtube.com/watch?v=AKCPPvaz8tU&t=1955s) & [Blog](https://blog.floydhub.com/knowledge-distillation/)
Explaining Knowledge Distillation By Quantifiying The Knowledge | [Video](https://www.youtube.com/watch?v=IBfQfuKA0Y8)
Self-training with Noisy Student improves ImageNet classification | [Paper](https://arxiv.org/abs/1911.04252)
How to avoid machine learning pitfalls: a guide for academic researchers | [Paper](https://arxiv.org/pdf/2108.02497.pdf)
OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER | [Paper](https://arxiv.org/abs/1701.06538)
