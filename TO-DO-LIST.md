Topic | Link | Done
------------|----- | ----- 
Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation | [Paper](https://arxiv.org/abs/1905.08094) | 
Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning | [Paper](https://arxiv.org/pdf/2012.09816.pdf),[Blog](https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/)
Pruning | 
Modern Math for Deep Learning | [Chapter](https://arxiv.org/pdf/2105.04026.pdf)
What exactly happens when we fine-tune BERT? | [Blog](https://medium.com/towards-data-science/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76)
Quantization|
